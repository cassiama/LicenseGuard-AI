import asyncio
import os
import traceback
import httpcore
import httpx
from datetime import datetime
from typing import Annotated, List, Optional
from dotenv import load_dotenv
from fastapi import FastAPI, Form, UploadFile, File, Header, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

try:
    from agent.graph import agent as agent_graph
    from agent.graph import AgentState
except ImportError:
    # should handle the case where the file is run directly
    from agent import graph as agent_graph, AgentState

# import the environment variables and set up the MCP server
# TODO: add a Pydantic Settings setup so that we're not using `python-dotenv` anymore
load_dotenv()
MCP_SERVER_HOST = os.getenv("MCP_SERVER_HOST", "http://localhost")
MCP_SERVER_PORT = os.getenv("MCP_SERVER_PORT", "8000")
MCP_URL = f"{MCP_SERVER_HOST}:{MCP_SERVER_PORT}/mcp"

app = FastAPI()


# allows the frontend to make requests to this REST API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    # TODO: for production, change to be specifically the frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class AnalysisRequest(BaseModel):
    project_name: str
    requirements_content: str


async def generate_final_report_from_agent(
    initial_state: AgentState,
) -> Optional[List[str]]:
    """
    Calls the agent graph, generates the report, and returns all of the chunks generated by the agent graph.

    :param initial_state: the initial state of the internal agent graph
    :type initial_state: AgentState
    :return: either the chunks of the final report generated by the agent, or it throws an exception
    :rtype: List[str] | None
    """
    chunks = []
    try:
        print(f"[{datetime.now()}]: Generating the final report from the agent...")
        # get detailed events and filter for just the 'final_report' chunk from the 'summarize' node
        async for event in agent_graph.astream_events(
            initial_state,
            version="v2",
        ):
            kind = event["event"]
            if kind == "on_chat_model_stream":
                # retrieve each token of the response
                chunk = event["data"]["chunk"]
                print(chunk)
                chunks.append(chunk.content)

        print(f"[{datetime.now()}]: Finished generating the final report.")
        return chunks
    except ExceptionGroup as eg:
        # check if any exception in the group is a connection error
        # prioritize connection errors (e.g. HTTP 503) over generic errors (e.g. HTTP 500)
        # TODO: if there are more errors than just connection or generic errors, come up 
        # with a more robust way handle multiple exceptions so that this doesn't become a 
        # bunch of if-else statements
        has_connect_error = any(
            isinstance(exc, (httpx.ConnectError, httpcore.ConnectError))
            for exc in eg.exceptions
        )
        
        if has_connect_error:
            print(
                f"[{datetime.now()}]: Connection to MCP server failed. Ending response immediately."
            )
            raise HTTPException(
                status_code=503,
                detail="Unable to connect to the MCP server. Please try again later.",
            )
        else:
            print(
                f'[{datetime.now()}]: An unknown error occurred ("{str(eg)}"). Ending response immediately.'
            )
            traceback.print_exc()
            raise HTTPException(
                status_code=500,
                detail="An unknown error occurred while generating the report. Please try again later.",
            )


async def stream_agent_response(chunks: List[str]):
    """
    With the given response chunks, it streams the final report.

    :param chunks: the chunks returned from the internal agent
    :type chunks: List[str]
    """
    print(f"[{datetime.now()}]: Starting the agent's streaming response.")

    for chunk in chunks:
        yield chunk
        await asyncio.sleep(0.05)  # slight delay to simulate streaming

    print(f"[{datetime.now()}]: Stream complete.")


@app.post("/generate/report")
async def generate_report(
    requirements_file: Annotated[
        UploadFile, File(description="A requirements.txt file (text/plain).")
    ],
    project_name: Annotated[str, Form(description="The name of the project")],
    authorization: str = Header(..., description="The user's Bearer token"),
):
    """
    Accepts a requirements.txt file upload, a project name & the user's authentication token, analyzes each license associated with the dependencies in the 'requirements.txt' file, and initiates an intelligent analysis of the requirements.txt file.

    Throws a 401 if:
        - the authorization header is missing.
        - the authorization header isn't in the correct form.

    Keyword arguments:

    file -- an non-empty 'requirements.txt'

    project_name -- the name of your project

    authorization -- a header that contains the user's Bearer token
    """
    if not authorization:
        raise HTTPException(status_code=401, detail="Authorization header is missing.")
    if not authorization.startswith("Bearer "):
        raise HTTPException(
            status_code=401,
            detail="Authorization header should be in the form of 'Bearer <token>'.",
        )

    requirements_content: str = (await requirements_file.read()).decode("utf-8")

    # initialize the internal agent with an empty JSON & report, the requirements.txt &
    # project name, and the user's auth token
    # NOTE: passing in the user's auth token is a TEMPORARY solution; token passthrough is
    # heavily frowned upon (source: https://modelcontextprotocol.io/specification/2025-06-18/basic/security_best_practices#token-passthrough)
    initial_state = AgentState(
        project_name=project_name,
        requirements_content=requirements_content,
        auth_token=authorization.split(" ")[
            1
        ],  # this gets the token from the Authorization header
        analysis_json={},
        final_report="",
    )

    # generate the response chunks from the internal agent
    chunks: Optional[List[str]] = []
    try:
        chunks = await generate_final_report_from_agent(initial_state)
        # if retrieving the response chunks from the agent completely failed, then raise a HTTP
        # 500 status code error
        if chunks is None or chunks == []:
            raise HTTPException(
                status_code=500,
                detail="An unknown error occurred while generating the report. Please try again later.",
            )
    except HTTPException as e:
        raise e

    # return a streaming response using the final report chunks generated from the internal agent
    return StreamingResponse(
        stream_agent_response(chunks),
        media_type="text/plain",
        headers={
            "X-Accel-Buffering": "no",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


if __name__ == "__main__":
    pass
